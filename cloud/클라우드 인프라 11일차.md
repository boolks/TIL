### 스웜 모드 볼륨

도커 데몬 명령어 중 run  명령어에서 -v 옵션을 사용할 때는 호스트와 디렉토리를 공유하는 경우와, 볼륨을 사용하는 경우에 대한 구분이 없음.

스웜 모드에서는 서비스를 생성할 때 도커 볼륨을 쓸지, 호스트와 디렉토리를 공유할지 명확하게 표시

##### volume 타입의 볼륨 생성

```
vagrant@swarm-manager:~$ docker service create --name ubuntu --mount type=volume,source=myvol,target=/root ubuntu:14.04 ping docker.com

--mount 옵션의 type 값에 volume을 지정 => 도커 볼륨을 사용하는 서비스 생성
source => 사용할 볼륨
target => 컨테이너 내부에 마운트될 디렉토리 위치
```

##### source 옵션을 명시하지 않으면 익명의 이름을 가진 볼륨 생성

```
vagrant@swarm-manager:~$ vagrant@swarm-manager:~$ docker service create --name ubuntu3 --mount type=volume,target=/etc/vim/ ubuntu:14.04 ping docker.com
```

##### 서비스 컨테이너에서 볼륨에 공유할 컨테이너의 디렉토리 파일이 이미 존재하면 이 파일들은 볼륨에 복사되고, 호스트에서 별도의 공간을 차지함

```
vagrant@swarm-manager:~$ docker service create --name ubuntu --mount type=volume,source=test,target=/etc/vim/ ubuntu:14.04 ping docker.com

vagrant@swarm-worker1:~$ docker run -it --name test -v test:/root ubuntu:14.04

root@81ee6f178a41:/# ls root/
```

##### 생성된 서비스, 컨테이너, 볼륨 삭제

```
vagrant@swarm-manager:~$ docker service rm $(docker service ls -q)
vagrant@swarm-manager:~$ docker container rm -f $(docker container ls -q)
vagrant@swarm-manager:~$ docker volume rm -f $(docker volume ls -q)
```

##### 서비스를 생성할 볼륨 옵션에 volume-nocopy를 추가하면 컨테이너의 파일이 볼륨에 복사되지 않음

```
vagrant@swarm-manager:~$ docker service create --name ubuntu --mount type=volume,source=test,target=/etc/vim/,volume-nocopy ubuntu:14.04 ping docker.com

vagrant@swarm-manager:~$ docker run -it --name test -v test:/root ubuntu:14.04
root@cb05f7b4a737:/# ls -al root/
total 16
drwx------ 2 root root 4096 Sep 18 01:48 .
drwxr-xr-x 1 root root 4096 Sep 18 01:48 ..
-rw-r--r-- 1 root root 3106 Feb 20  2014 .bashrc
-rw-r--r-- 1 root root  140 Feb 20  2014 .profile

```

##### bind 타입의 볼륨 생성

바인드 타입은 호스트와 디렉토리 공유

type 옵션의 값을 bind로 결정

볼륨 타입과 달리 공유될 호스트의 디렉토리를 설정해야 하므로 source 옵션을 반드시 명시

호스트의 /root/host 디렉토리를 서비스 컨테이너의 /root/container 디렉토리에 마운트

```
vagrant@swarm-manager:~$ docker service create --name ubuntu --mount type=bind,src=/home/vagrant/host,dst=/root/container ubuntu:14.04 ping docker.com

vagrant@swarm-manager:~/host$ docker service ps ubuntu
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE                 ERROR                              PORTS
64uyc0f216b9        ubuntu.1            ubuntu:14.04        swarm-manager       Running             Running about a minute ago

vagrant@swarm-manager:~/host$ docker container ls
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
3d10321e3354        ubuntu:14.04        "ping docker.com"   2 minutes ago       Up 2 minutes                            ubuntu.1.64uyc0f216b9vwmguodwbs8ub
vagrant@swarm-manager:~/host$ docker container exec -it ubuntu.1.64uyc0f216b9vwmguodwbs8ub /bin/bash
root@3d10321e3354:/# cd /root/container
root@3d10321e3354:~/container# ls
hello
root@3d10321e3354:~/container# touch hello_in_container

vagrant@swarm-manager:~/host$ ls
hello  hello_in_container
```

### 도커 스웜 노드 다루기

##### 구축한 스웜 클러스터의 노드 상태 확인

```
docker node ls
```

##### Active 상태

```
docker node update --availability active node_name
```

새로운 노드가 스웜 클러스터에 추가되면 기본적으로 설정되는 상태

노드가 서비스의 컨테이너를 할당 받을 수 있음

##### Drain 상태

```
docker node update --availability drain node_name
```

스웜 매니저의 스케줄러는 컨테이너를 해당 노드에 할당하지 않음

일반적으로 매니저 노드 또는 노드에 문제가 생겨 일시적으로 사용하지 않는 상태로 설정

Drain 상태가 되면 해당 노드에서 실행중이던 서비스의 컨테이너는 전부 중지

다시 Active로 변경한다고 해서 컨테이너가 다시 분산되어 할당하지 않으므로

docker service scale 명령어를 사용해 컨테이너의 균형을 재조정해야 함 

```
vagrant@swarm-manager:~/host$ docker node update --availability drain swarm-worker2
swarm-worker2
vagrant@swarm-manager:~/host$ docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
ojrpa9kyygdm5hxi3z40ku2m9 *   swarm-manager       Ready               Active              Leader              19.03.6
06glastb0e56wk9pm4gw0nfwg     swarm-worker1       Ready               Active                                  19.03.6
op0tv03dazlvbogkvra0be0d0     swarm-worker2       Ready               Drain                                   19.03.6
vagrant@swarm-manager:~/host$ docker service create --name nginx --replicas 5 nginx
image nginx:latest could not be accessed on a registry to record
its digest. Each node will access nginx:latest independently,
possibly leading to different nodes running different
versions of the image.

bjs0h67g8k5h0jkg54kpuz04d
overall progress: 5 out of 5 tasks
1/5: running
2/5: running
3/5: running
4/5: running
5/5: running
verify: Service converged

vagrant@swarm-manager:~/host$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
bjs0h67g8k5h        nginx               replicated          5/5                 nginx:latest
7fk9rbupc3ho        ubuntu              replicated          1/1                 ubuntu:14.04

vagrant@swarm-manager:~/host$ docker service ps nginx
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR                           PORTS
ohs19hkgek7a        nginx.1             nginx:latest        swarm-worker1       Running             Running 2 minutes ago
or8mngb9aff3        nginx.2             nginx:latest        swarm-manager       Running             Running 2 minutes ago
3ufof63tgph0        nginx.3             nginx:latest        swarm-worker1       Running             Running 2 minutes ago
3vs8ngg7ir78         \_ nginx.3         nginx:latest        swarm-worker1       Shutdown            Rejected 2 minutes ago   "No such image: nginx:latest"
mooa0p3488ia         \_ nginx.3         nginx:latest        swarm-worker1       Shutdown            Rejected 2 minutes ago   "No such image: nginx:latest"
sz7g7p0vxzox        nginx.4             nginx:latest        swarm-worker1       Running             Running 2 minutes ago
ii8g060mp6lg        nginx.5             nginx:latest        swarm-manager       Running             Running 2 minutes ago

activate 상태로 전환
vagrant@swarm-manager:~/host$ docker node update --availability active swarm-worker2
swarm-worker2

스케일 재조정
vagrant@swarm-manager:~/host$ docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
ojrpa9kyygdm5hxi3z40ku2m9 *   swarm-manager       Ready               Active              Leader              19.03.6
06glastb0e56wk9pm4gw0nfwg     swarm-worker1       Ready               Active                                  19.03.6
op0tv03dazlvbogkvra0be0d0     swarm-worker2       Ready               Active                                  19.03.6

scale down
vagrant@swarm-manager:~/host$ docker service scale nginx=1
nginx scaled to 1
overall progress: 1 out of 1 tasks
1/1: No such image: nginx:latest
verify: Service converged
vagrant@swarm-manager:~/host$ docker service ps nginx
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR                           PORTS
ohs19hkgek7a        nginx.1             nginx:latest        swarm-worker1       Running             Running 4 minutes ago
3vs8ngg7ir78        nginx.3             nginx:latest        swarm-worker1       Shutdown            Rejected 4 minutes ago   "No such image: nginx:latest"
mooa0p3488ia         \_ nginx.3         nginx:latest        swarm-worker1       Shutdown            Rejected 4 minutes ago   "No such image: nginx:latest"

scale up
vagrant@swarm-manager:~/host$ docker service scale nginx=5
nginx scaled to 5
overall progress: 5 out of 5 tasks
1/5: running
2/5: running
3/5: running
4/5: running
5/5: running
verify: Service converged
vagrant@swarm-manager:~/host$ docker service ps nginx
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE                 ERROR                           PORTS
ohs19hkgek7a        nginx.1             nginx:latest        swarm-worker1       Running             Running 6 minutes ago
uca1gqvmaoqk        nginx.2             nginx:latest        swarm-manager       Running             Running about a minute ago
w65vp0y7qzlo        nginx.3             nginx:latest        swarm-worker1       Running             Running about a minute ago
3vs8ngg7ir78         \_ nginx.3         nginx:latest        swarm-worker1       Shutdown            Rejected 7 minutes ago        "No such image: nginx:latest"
mooa0p3488ia         \_ nginx.3         nginx:latest        swarm-worker1       Shutdown            Rejected 7 minutes ago        "No such image: nginx:latest"
w0m33vp2j0id        nginx.4             nginx:latest        swarm-worker2       Running             Running about a minute ago
l7q0nrabz1e7         \_ nginx.4         nginx:latest        swarm-worker2       Shutdown            Rejected about a minute ago   "No such image: nginx:latest"
lx4z0bi2k6uu        nginx.5             nginx:latest        swarm-worker2       Running             Running about a minute ago

Active 상태인 노드에 컨테이너가 골고루 배치된 것을 확인
```

##### Pause 상태

docker node update --availability pause node_name

서비스의 컨테이너를 더는 할당받지 않지만 실행중인 컨테이너가 중지되지 않음

##### 노드 라벨 추가

특정 노드에 라벨을 추가하면 서비스를 할당할 때 컨테이너를 생성할 노드의 그룹을 선택하는 것이 가능

```
vagrant@swarm-manager:~/host$ docker node update --label-add storage=ssd swarm-worker1
swarm-worker1
vagrant@swarm-manager:~/host$ docker node inspect --pretty swarm-worker1
ID:                     06glastb0e56wk9pm4gw0nfwg
Labels:
 - storage=ssd

```

#### 서비스 제약 설정

docker service create 명령어에 --constraint 옵션을 추가해 서비스의 컨테이너가 할당될 노드의 종류 선택 가능

##### 1. node.labels 제약조건

storage 키의 값이 ssd로 설정된 노드에 서비스의 컨테이너를 할당하도록 제한

```
vagrant@swarm-manager:~/host$ docker service create --name label_test --constraint 'node.labels.storage == ssd' --replicas=5 ubuntu:14.04 ping docker.com

vagrant@swarm-manager:~/host$ docker service ps label_test
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
b855osyvlunc        label_test.1        ubuntu:14.04        swarm-worker1       Running             Running 25 seconds ago
kgyl4lqc2665        label_test.2        ubuntu:14.04        swarm-worker1       Running             Running 25 seconds ago
x2odms2015dr        label_test.3        ubuntu:14.04        swarm-worker1       Running             Running 26 seconds ago
l0iuq10kqnr8        label_test.4        ubuntu:14.04        swarm-worker1       Running             Running 26 seconds ago
807fe23f2cwg        label_test.5        ubuntu:14.04        swarm-worker1       Running             Running 26 seconds ago

storage 라벨이 ssd로 설정된 swarm-worker1 노드에만 컨테이너가 생성되어있음
제약조건을 만족하는 노드를 찾지 못할 경우에는 서비스의 컨테이너가 생성되지 않음
```

##### node.id 제약조건

노드 ID를 명시해 서비스의 컨테이너를 할당할 노드 선택

다른 도커 명령어와 달리 docker node ls 명령어에 출력된 ID를 전부 입력해야 함

```
vagrant@swarm-manager:~/host$ docker node ls | grep swarm-worker2
op0tv03dazlvbogkvra0be0d0     swarm-worker2       Ready               Active                                  19.03.6
vagrant@swarm-manager:~/host$ docker service create --constraint 'node.id == op0tv03dazlvbogkvra0be0d0' --replicas=5 --name label_test2 ubuntu:14.04 ping docker.com
2zuce4em43u4udfdg61a6rl7v
overall progress: 5 out of 5 tasks
1/5: running
2/5: running
3/5: running
4/5: running
5/5: running
verify: Service converged
vagrant@swarm-manager:~/host$ docker service ps label_test2
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
jidtp5s43mfi        label_test2.1       ubuntu:14.04        swarm-worker2       Running             Running 28 seconds ago
tbn85wu8gzpv        label_test2.2       ubuntu:14.04        swarm-worker2       Running             Running 28 seconds ago
hiyax6a9kfmu        label_test2.3       ubuntu:14.04        swarm-worker2       Running             Running 28 seconds ago
p52r7tsjqb7m        label_test2.4       ubuntu:14.04        swarm-worker2       Running             Running 28 seconds ago
gtg8rvujqltj        label_test2.5       ubuntu:14.04        swarm-worker2       Running             Running 28 seconds ago
```

##### 3. node.hostname과 node.role 제약조건

스웜 클러스터에 등록된 호스트 이름 및 역할로 제한 조건 설정

```
hostname이 swarm-worker1인 노드에 생성
vagrant@swarm-manager:~/host$ docker service create --constraint 'node.hostname == swarm-worker1' --name label_test3 --replicas=5 ubuntu:14.04 ping docker.com
os7e0pou7qej2b3ozqq2igvbi
overall progress: 5 out of 5 tasks
1/5: running
2/5: running
3/5: running
4/5: running
5/5: running
verify: Service converged
vagrant@swarm-manager:~/host$ docker service ps label_test3
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
hus06rffc3q0        label_test3.1       ubuntu:14.04        swarm-worker1       Running             Running 10 seconds ago
3ayaarhi7ws2        label_test3.2       ubuntu:14.04        swarm-worker1       Running             Running 10 seconds ago
srmxfltjwmu5        label_test3.3       ubuntu:14.04        swarm-worker1       Running             Running 10 seconds ago
oozvvyaeu3lc        label_test3.4       ubuntu:14.04        swarm-worker1       Running             Running 10 seconds ago
yfsqe0x02gn4        label_test3.5       ubuntu:14.04        swarm-worker1       Running             Running 10 seconds ago

role이 manager가 아닌 노드에 생성
vagrant@swarm-manager:~/host$ docker service create --constraint 'node.role != manager' --name label_test4 --replicas=5 ubuntu:14.04 ping docker.com
no4gqlp7rlah7k55g9nxxdnhg
overall progress: 5 out of 5 tasks
1/5: running
2/5: running
3/5: running
4/5: running
5/5: running
verify: Service converged
vagrant@swarm-manager:~/host$ docker service ps label_test4
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
iqfoi90wtuct        label_test4.1       ubuntu:14.04        swarm-worker2       Running             Running 11 seconds ago
fq4hhv1yh1ab        label_test4.2       ubuntu:14.04        swarm-worker1       Running             Running 12 seconds ago
wx906ynabver        label_test4.3       ubuntu:14.04        swarm-worker2       Running             Running 11 seconds ago
4xndd03qdh1j        label_test4.4       ubuntu:14.04        swarm-worker2       Running             Running 12 seconds ago
17vvm2hndyqx        label_test4.5       ubuntu:14.04        swarm-worker1       Running             Running 12 seconds ago
vagrant@swarm-manager:~/host$
```

##### 4, engine.labels 제약조건

도커 엔진, 즉 도커 데몬 자체에 라벨을 설정해 제한 조건을 설정

```
각 노드의 도커 데몬 실행 옵션 변경
DOCKER_OPTS="... --label=mylabel=worker2 --label=mylabel2=second_worker ..."

vagrant@swarm-manager:~/host$ docker info

vagrant@swarm-manager:~/host$ docker service create --constraint 'engine.labels.mylabel == worker2' --name engine_label --replicas=5 ubuntu:14.04 ping docker.com

2개의 제약 조건을 동시에 설정
vagrant@swarm-manager:~/host$ docker service create --name engine_label --replicas=5 \
> --constraint 'engine.labels.mylabel == worker2' \
> --constraint 'engine.labels.mylabel2 = second_worker' \
> ubuntu:14.04 ping docker.com
```

### Kubernetes

##### 가상 머신 생성

```
Vagrantfile
# -*- mode: ruby -*-
# vi: set ft=ruby :

Vagrant.configure("2") do |config|
  config.vm.box = "ubuntu/bionic64"
  config.vm.hostname = "ubuntu"
  config.vm.network "private_network", ip: "192.168.111.110"
  config.vm.synced_folder ".", "/home/vagrant/sync", disabled: true
  config.vm.provider "virtualbox" do |vb|
    vb.cpus = 2
    vb.memory = 2048
  end
end

vagrant up
vagrant ssh

패키지 최신화
vagrant@ubuntu:~$ sudo su
root@ubuntu:/home/vagrant# cd
root@ubuntu:~# apt update
root@ubuntu:~# apt upgrade

도커 설치 및 설정
root@ubuntu:~# apt install docker.io -y
root@ubuntu:~# usermod -a -G docker vagrant
root@ubuntu:~# service docker restart
root@ubuntu:~# chmod 666 /var/run/docker.sock

root@ubuntu:~# docker version
Client:
 Version:           19.03.6
 API version:       1.40
 Go version:        go1.12.17
 Git commit:        369ce74a3c
 Built:             Fri Feb 28 23:45:43 2020
 OS/Arch:           linux/amd64
 Experimental:      false

Server:
 Engine:
  Version:          19.03.6
  API version:      1.40 (minimum version 1.12)
  Go version:       go1.12.17
  Git commit:       369ce74a3c
  Built:            Wed Feb 19 01:06:16 2020
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          1.3.3-0ubuntu1~18.04.2
  GitCommit:
 runc:
  Version:          spec: 1.0.1-dev
  GitCommit:
 docker-init:
  Version:          0.18.0
  GitCommit:
```

##### kubectl 설치

```
root@ubuntu:~# apt-get update && sudo apt-get install -y apt-transport-https gnupg2
root@ubuntu:~# curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
root@ubuntu:~# echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list
root@ubuntu:~# apt-get update
root@ubuntu:~# apt-get install -y kubectl
root@ubuntu:~# kubectl version
```

##### Minikube 설치

```
root@ubuntu:~# curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube
root@ubuntu:~# mkdir -p /usr/local/bin/
root@ubuntu:~# install minikube /usr/local/bin/
```

##### 클러스터 시작

```
vagrant@ubuntu:~$ minikube start
```

##### 버전 확인

```
Kubernetes
vagrant@ubuntu:~$ kubectl version
Client Version: version.Info{Major:"1", Minor:"19", GitVersion:"v1.19.2", GitCommit:"f5743093fd1c663cb0cbc89748f730662345d44d", GitTreeState:"clean", BuildDate:"2020-09-16T13:41:02Z", GoVersion:"go1.15", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"19", GitVersion:"v1.19.0", GitCommit:"e19964183377d0ec2052d1f1fa930c4d7575bd50", GitTreeState:"clean", BuildDate:"2020-08-26T14:23:04Z", GoVersion:"go1.15", Compiler:"gc", Platform:"linux/amd64"}

minikube
vagrant@ubuntu:~$ minikube status
minikube
type: Control Plane
host: Running
kubelet: Running
apiserver: Running
kubeconfig: Configured
```

### Kubernetes

- 대부분의 리소스를 "오브젝트" 라고 불리는 형태로 관리
- 쿠버네티스 에서는 컨테이너의 집합 (pods), 컨테이너 집합을 관리하는 컨트롤러 (replica set), 사용자 (service account), 노드 (node) 까지도 하나의 오브젝트로 사용할 수 있음
- kubectl 명령어 또는 yaml 파일을 정의해서 사용

#### 쿠버네티스 구성

![img](https://blog.kakaocdn.net/dn/m5z7P/btqIX3f1q0p/LgbpK7L7FlK3K8uicXPPCK/img.png)

##### 마스터 노드

전체 쿠버네티스 시스템을 제어하고 관리하는 쿠버네티스 컨트롤 플레인 (control plane)을 실행



##### 워커 노드

실제 배포되는 컨테이너 애플리케이션을 실행



##### 포드(pod)

컨테이너 애플리케이션의 기본 단위

1개 이상의 컨테이너로 구성된 컨테이너의 집합

여러 개의 컨테이너를 추상화해서 하나의 애플리케이션으로 동작하도록 묶어 놓은 컨테이너의 묶음

**nginx 컨테이너로 구성된 포드 생성**

```
vagrant@ubuntu:~/kub01$ vi nginx-pod.yml
apiVersion: v1						<= YAML 파일에서 정의한 오브젝트의 API 버전
kind: Pod							<= 리소스의 종류 (kubectl api-resources 명령의 KIND 항목)
metadata:							<= 라벨, 주석, 이름과 같은 리소스의 부가 정보
  name: my-nginx-pod
spec:								<= 리소스 생성을 위한 정보
  containers:
  - name: my-nginx-container
    image: nginx:latest
    ports:
    - containerPort: 80
      protocol: TCP
```

##### 새로운 pod 생성 및 확인

```
vagrant@ubuntu:~/kub01$ kubectl apply -f nginx-pod.yml
pod/my-nginx-pod created

vagrant@ubuntu:~/kub01$ kubectl get pods
NAME           READY   STATUS    RESTARTS   AGE
my-nginx-pod   1/1     Running   0          36s
```

##### 생성된 리소스의 자세한 정보를 확인

```
vagrant@ubuntu:~/kub01$ kubectl describe pods my-nginx-pod
Name:         my-nginx-pod
Namespace:    default
Priority:     0
Node:         minikube/172.17.0.3
Start Time:   Fri, 18 Sep 2020 06:35:12 +0000
Labels:       <none>
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:  172.18.0.3
Containers:
  my-nginx-container:
    Container ID:   docker://a5b3305d73df705c22c821ea4576e675d520fb206886fc6efa462d60034e9676
    Image:          nginx:latest
    Image ID:       docker-pullable://nginx@sha256:c628b67d21744fce822d22fdcc0389f6bd763daac23a6b77147d0712ea7102d0
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Fri, 18 Sep 2020 06:35:24 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-jdhcz (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  default-token-jdhcz:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-jdhcz
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  2m23s  default-scheduler  Successfully assigned default/my-nginx-pod to minikube
  Normal  Pulling    2m22s  kubelet            Pulling image "nginx:latest"
  Normal  Pulled     2m11s  kubelet            Successfully pulled image "nginx:latest" in 10.735566572s
  Normal  Created    2m11s  kubelet            Created container my-nginx-container
  Normal  Started    2m11s  kubelet            Started container my-nginx-container
```

##### 클러스터 내부에서 테스트를 위한 임시 pod를 생성해서 nginx pod의 동작을 확인

```
alicek106/ubuntu:curl 이미지를 이용해서 debug 이름의 pod 생성
vagrant@ubuntu:~/kub01$ kubectl run -it --rm debug --image=alicek106/ubuntu:curl --restart=Never bash
root@debug:/#

디버그 pod이 생성된 상태에서 조회
vagrant@ubuntu:~$ kubectl get pods
NAME           READY   STATUS    RESTARTS   AGE
debug          1/1     Running   0          76s
my-nginx-pod   1/1     Running   0          8m29s

디버그 pod에서 my-nginx-pod으로 요청 전달
root@debug:/# curl 172.18.0.3
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```

##### kubectl exec 명령으로 pod의 컨테이너에 명령어 전달

```
vagrant@ubuntu:~/kub01$ kubectl exec -it my-nginx-pod 명령어
vagrant@ubuntu:~/kub01$ kubectl exec -it my-nginx-pod -- bash
root@my-nginx-pod:/#
```

##### kubectl logs 명령으로 pod의 로그 확인

```
vagrant@ubuntu:~/kub01$ kubectl logs my-nginx-pod
/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
10-listen-on-ipv6-by-default.sh: Getting the checksum of /etc/nginx/conf.d/default.conf
10-listen-on-ipv6-by-default.sh: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Configuration complete; ready for start up
172.18.0.4 - - [18/Sep/2020:06:46:28 +0000] "GET / HTTP/1.1" 200 612 "-" "curl/7.35.0" "-"
```

##### 쿠버네티스 오브젝트를 삭제

```
vagrant@ubuntu:~/kub01$ kubectl delete -f nginx-pod.yml
pod "my-nginx-pod" deleted
vagrant@ubuntu:~/kub01$ kubectl get pods
No resources found in default namespace.
```

##### 새 pod 정의

```
vagrant@ubuntu:~/kub01$ vi nginx-pod-with-ubuntu.yml
apiVersion: v1
kind: Pod
metadata:
  name: my-nginx-pod
spec:
  containers:
  - name: my-nginx-container
    image: nginx:latest
    ports:
    - containerPort: 80
      protocol: TCP

  - name: ubuntu-sidecar-container
    image: alicek106/rr-test:curl
    command: [ "tail" ]
    args: [ "-f", "/dev/null" ]
    
vagrant@ubuntu:~/kub01$ kubectl get pods
NAME           READY   STATUS    RESTARTS   AGE
my-nginx-pod   2/2     Running   0          21s

```

##### pod로 명령어를 전달할 때 실행할 컨테이너를 지정하지 않으면 default로 설정된 컨테이너가 실행됨

##### 특정 컨테이너에게 명령어를 전달할 때는 -c 옵션 사용

```
vagrant@ubuntu:~/kub01$ kubectl exec -it my-nginx-pod -- bash
Defaulting container name to my-nginx-container.
Use 'kubectl describe pod/my-nginx-pod -n default' to see all of the containers in this pod.
root@my-nginx-pod:/# exit
exit
vagrant@ubuntu:~/kub01$ kubectl exec -it my-nginx-pod -c ubuntu-sidecar-container -- bash
```

##### ubuntu-sidecar-container 만으로 구성된 pod를 생성한 후 해당 pod 내에서 curl을 동작

```
vagrant@ubuntu:~/kub01$ vi nginx-pod-test.yml
apiVersion: v1
kind: Pod
metadata:
  name: my-nginx-pod-test
spec:
  containers:
  - name: ubuntu-sidecar-container
    image: alicek106/rr-test:curl			<= 웹 서버를 포함하고 있지 않음
    command: [ "tail" ]
    args: [ "-f", "/dev/null" ]

vagrant@ubuntu:~/kub01$ kubectl exec -it my-nginx-pod-test -- bash
root@my-nginx-pod-test:/# curl localhost
curl: (7) Failed to connect to localhost port 80: Connection refused
=> 80 포트로 서비스를 제공하고 있지 않다
```

※ 포드 내부의 컨테이너들은 네트워크와 같은 리눅스 네임스페이스를 공유



#### 컨트롤 플레인 (Control Plane)

클러스터를 제어하고 작동시킴

애플리케이션을 실행하지는 않음 => 노드에서 처리

구성요소

- 쿠버네티스 API 서버 - 사용자 컨트롤 플레인 구성 요소와 통신
- 스케줄러 - 애플리케이션의 배포를 담당 (애플리케이션의 배포 가능한 각 구성 요소를 워크 노드에 할당)
- 컨트롤러 매니저 - 구성 요소 복제본, 워커 노드 추적, 노드 장애처리 등과 같은 클러스터 단의 기능 수행
- etcd - 클러스터 구성을 지속적으로 저장하는 신뢰할 수 있는 분산 데이터 저장소



#### 워커 노드 (Worker Node)

컨테이너화 된 애플리케이션을 실행하는 시스템

애플리케이션을 실행하고 모니터링 하며 애플리케이션에 서비스 제공

구성요소

- 컨테이너를 실행하는 도커, rkt 또는 다른 컨테이너 런타임 (container runtime)
- API 서버와 통신하고 노드의 컨테이너를 관리하는 kubelet
- 애플리케이션 구성 요소 간에 네트워크 트래픽을 로드밸런싱하는 쿠버네티스 서비스 프록시